#!/usr/bin/env python3
"""
æ¨ç†è„šæœ¬

æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
1. åŸå§‹æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨ä¸‰æ¨¡æ€å›¾åƒè¾“å…¥
2. TMIç‰¹å¾æ¨¡å¼ï¼šä½¿ç”¨é¢„æå–çš„TMIç‰¹å¾ï¼ˆLLaMA Factoryè®­ç»ƒåçš„æ¨¡å‹ï¼‰

åŠŸèƒ½ï¼š
- å•æ ·æœ¬å’Œæ‰¹é‡æ¨ç†
- 6æ‘„åƒå¤´å…¨æ™¯èåˆ
- äº¤äº’å¼æ¨ç†æ¨¡å¼
- è½¨è¿¹å¯è§†åŒ–
- ADE/FDEè¯„ä¼°æŒ‡æ ‡
"""

import os
import sys
import json
import argparse
import warnings
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union
import numpy as np
from datetime import datetime
from dataclasses import dataclass
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

try:
    import torch
    import torch.nn as nn
    from torch.utils.data import DataLoader
except ImportError as e:
    raise ImportError(f"ç¼ºå°‘å¿…è¦çš„PyTorchä¾èµ–: {e}")

try:
    from transformers import AutoTokenizer, AutoImageProcessor
except ImportError as e:
    raise ImportError(f"ç¼ºå°‘transformersä¾èµ–: {e}")

try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    from PIL import Image
    import cv2
except ImportError:
    warnings.warn("å¯è§†åŒ–åŠŸèƒ½éœ€è¦matplotlibã€seabornã€PILå’Œopencv")
    plt = None
    sns = None
    Image = None
    cv2 = None

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from src.tri_modal_qwen.modeling.configuration_tri_modal_qwen import TriModalQwenConfig
    from src.tri_modal_qwen.modeling.modeling_tri_modal_qwen import TriModalQwenForCausalLM
    from src.tri_modal_qwen.data.processor import TriModalProcessor
    from src.tri_modal_qwen.utils.visualization import TriModalVisualizer
except ImportError as e:
    raise ImportError(f"æ— æ³•å¯¼å…¥é¡¹ç›®æ¨¡å—: {e}")


@dataclass
class InferenceConfig:
    """
    æ¨ç†é…ç½®
    """
    # æ¨¡å‹é…ç½®
    model_path: str = "./checkpoints/best_model"
    model_config_path: Optional[str] = None
    use_tmi_features: bool = False  # æ˜¯å¦ä½¿ç”¨TMIç‰¹å¾æ¨¡å¼
    tmi_feature_path: Optional[str] = None  # TMIç‰¹å¾æ–‡ä»¶è·¯å¾„
    tmi_checkpoint: Optional[str] = None  # TMIæ¨¡å—checkpointï¼ˆç”¨äºåœ¨çº¿æå–ç‰¹å¾ï¼‰
    
    # è¾“å…¥é…ç½®
    rgb_image_path: Optional[str] = None
    depth_image_path: Optional[str] = None
    semantic_image_path: Optional[str] = None
    text_prompt: str = "åŸºäºä¸‰æ¨¡æ€æ„ŸçŸ¥ä¿¡æ¯ï¼Œé¢„æµ‹è½¦è¾†çš„æœªæ¥è½¨è¿¹ã€‚"
    
    # æ‰¹é‡æ¨ç†é…ç½®
    batch_input_dir: Optional[str] = None
    output_dir: str = "./inference_results"
    
    # ç”Ÿæˆé…ç½®
    max_new_tokens: int = 512
    temperature: float = 0.0
    top_p: float = 1.0
    do_sample: bool = False
    num_beams: int = 1
    repetition_penalty: float = 1.0
    
    # å¤„ç†é…ç½®
    max_length: int = 2048
    image_size: int = 392
    
    # å¯è§†åŒ–é…ç½®
    save_visualization: bool = True
    show_attention: bool = False
    interactive_mode: bool = False
    
    # æ€§èƒ½é…ç½®
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    torch_dtype: str = "float16"  # float16, float32
    use_compile: bool = False  # PyTorch 2.0 compile
    
    # å…¶ä»–é…ç½®
    seed: int = 42
    verbose: bool = True


class TriModalInference:
    """
    ä¸‰æ¨¡æ€VLMæ¨ç†å™¨
    """
    
    def __init__(self, config: InferenceConfig):
        self.config = config
        self.model = None
        self.tokenizer = None
        self.processor = None
        self.visualizer = None
        
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(config.seed)
        np.random.seed(config.seed)
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        Path(config.output_dir).mkdir(parents=True, exist_ok=True)
        
        if config.verbose:
            print(f"æ¨ç†é…ç½®:")
            print(f"  æ¨¡å‹è·¯å¾„: {config.model_path}")
            print(f"  è¾“å‡ºç›®å½•: {config.output_dir}")
            print(f"  è®¾å¤‡: {config.device}")
            print(f"  æ•°æ®ç±»å‹: {config.torch_dtype}")
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹å’Œç›¸å…³ç»„ä»¶"""
        
        if self.config.verbose:
            print("åŠ è½½æ¨¡å‹...")
        
        # ç¡®å®šæ•°æ®ç±»å‹
        torch_dtype = getattr(torch, self.config.torch_dtype)
        
        # æ ¹æ®æ¨¡å¼é€‰æ‹©ä¸åŒçš„æ¨¡å‹åŠ è½½æ–¹å¼
        if self.config.use_tmi_features:
            # TMIç‰¹å¾æ¨¡å¼ï¼šåŠ è½½æ ‡å‡†Qwenæ¨¡å‹ + TMIæ”¯æŒ
            if self.config.verbose:
                print("ä½¿ç”¨TMIç‰¹å¾æ¨¡å¼ï¼ŒåŠ è½½æ ‡å‡†Qwen2.5-VLæ¨¡å‹...")
            
            try:
                # å°è¯•åŠ è½½LLaMA Factoryè®­ç»ƒçš„æ¨¡å‹
                from transformers import Qwen2VLForConditionalGeneration
                self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                    self.config.model_path,
                    torch_dtype=torch_dtype,
                    device_map="auto" if self.config.device == "cuda" else None,
                    trust_remote_code=True
                )
                
                # åŠ¨æ€æ³¨å…¥TMIæ”¯æŒ
                sys.path.append(str(Path(__file__).parent.parent / "llama_factory_configs"))
                from inject_tmi_to_qwen import inject_tmi_support
                self.model = inject_tmi_support(self.model, tmi_hidden_size=4096)
                
                if self.config.verbose:
                    print("âœ“ TMIæ”¯æŒå·²æ³¨å…¥åˆ°æ ‡å‡†Qwenæ¨¡å‹")
                    
            except Exception as e:
                if self.config.verbose:
                    print(f"è­¦å‘Š: æ— æ³•åŠ è½½Qwen2.5-VLï¼Œå›é€€åˆ°TriModalQwen: {e}")
                # å›é€€åˆ°åŸå§‹ä¸‰æ¨¡æ€æ¨¡å‹
                self.config.use_tmi_features = False
        
        if not self.config.use_tmi_features:
            # åŸå§‹æ¨¡å¼ï¼šåŠ è½½å®Œæ•´çš„ä¸‰æ¨¡æ€æ¨¡å‹
            # åŠ è½½é…ç½®
            if self.config.model_config_path:
                with open(self.config.model_config_path, 'r') as f:
                    model_config_dict = json.load(f)
                model_config = TriModalQwenConfig.from_dict(model_config_dict)
            else:
                config_path = Path(self.config.model_path) / "config.json"
                if config_path.exists():
                    model_config = TriModalQwenConfig.from_pretrained(self.config.model_path)
                else:
                    model_config = TriModalQwenConfig()
            
            # åŠ è½½æ¨¡å‹
            self.model = TriModalQwenForCausalLM.from_pretrained(
                self.config.model_path,
                config=model_config,
                torch_dtype=torch_dtype,
                device_map="auto" if self.config.device == "cuda" else None
            )
        
        if self.config.device != "cuda":
            self.model.to(self.config.device)
        
        self.model.eval()
        
        # ç¼–è¯‘æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        if self.config.use_compile and hasattr(torch, 'compile'):
            if self.config.verbose:
                print("ç¼–è¯‘æ¨¡å‹...")
            self.model = torch.compile(self.model)
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.model_path,
            trust_remote_code=True,
            padding_side='left'  # Flash Attentionè¦æ±‚
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        
        # åˆ›å»ºå¤„ç†å™¨
        self.processor = TriModalProcessor(
            tokenizer=self.tokenizer,
            image_processor=AutoImageProcessor.from_pretrained(
                self.config.model_path,
                trust_remote_code=True
            ),
            max_length=self.config.max_length
        )
        
        # åˆ›å»ºå¯è§†åŒ–å™¨
        if self.config.save_visualization:
            self.visualizer = TriModalVisualizer(
                model=self.model,
                processor=self.processor
            )
        
        if self.config.verbose:
            param_count = sum(p.numel() for p in self.model.parameters()) / 1e6
            print(f"æ¨¡å‹å‚æ•°é‡: {param_count:.1f}M")
        
        return self.model, self.tokenizer, self.processor
    
    def load_images(
        self, 
        rgb_paths: Union[str, List[str]], 
        depth_paths: Optional[Union[str, List[str]]] = None, 
        semantic_paths: Optional[Union[str, List[str]]] = None
    ) -> Dict[str, Union[np.ndarray, List[np.ndarray]]]:
        """åŠ è½½ä¸‰æ¨¡æ€å›¾åƒï¼ˆæ”¯æŒå•å›¾åƒæˆ–6æ‘„åƒå¤´ï¼‰"""
        
        if not Image:
            raise ImportError("éœ€è¦å®‰è£…PILåº“ç”¨äºå›¾åƒå¤„ç†")
        
        images = {}
        
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        if isinstance(rgb_paths, str):
            rgb_paths = [rgb_paths]
        if isinstance(depth_paths, str):
            depth_paths = [depth_paths]
        if isinstance(semantic_paths, str):
            semantic_paths = [semantic_paths]
        
        # åŠ è½½RGBå›¾åƒ
        rgb_images = []
        for rgb_path in rgb_paths:
            if rgb_path and Path(rgb_path).exists():
                rgb_image = Image.open(rgb_path).convert('RGB')
                rgb_image = rgb_image.resize((self.config.image_size, self.config.image_size))
                rgb_images.append(np.array(rgb_image))
            else:
                raise FileNotFoundError(f"RGBå›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {rgb_path}")
        images['rgb'] = rgb_images if len(rgb_images) > 1 else rgb_images[0]
        
        # åŠ è½½æ·±åº¦å›¾åƒ
        if depth_paths:
            depth_images = []
            for depth_path in depth_paths:
                if depth_path and Path(depth_path).exists():
                    depth_image = Image.open(depth_path).convert('L')  # ç°åº¦å›¾
                    depth_image = depth_image.resize((self.config.image_size, self.config.image_size))
                    depth_images.append(np.array(depth_image))
                else:
                    if self.config.verbose:
                        print(f"è­¦å‘Š: æ·±åº¦å›¾åƒä¸å­˜åœ¨: {depth_path}")
                    return None  # å¦‚æœç¼ºå°‘æ·±åº¦å›¾ï¼Œè¿”å›Noneè¡¨ç¤ºæ— æ³•å¤„ç†
            images['depth'] = depth_images if len(depth_images) > 1 else depth_images[0]
        
        # åŠ è½½è¯­ä¹‰åˆ†å‰²å›¾åƒ
        if semantic_paths:
            semantic_images = []
            for semantic_path in semantic_paths:
                if semantic_path and Path(semantic_path).exists():
                    semantic_image = Image.open(semantic_path).convert('L')  # ç°åº¦å›¾
                    semantic_image = semantic_image.resize((self.config.image_size, self.config.image_size))
                    semantic_images.append(np.array(semantic_image))
                else:
                    if self.config.verbose:
                        print(f"è­¦å‘Š: è¯­ä¹‰å›¾åƒä¸å­˜åœ¨: {semantic_path}")
                    return None  # å¦‚æœç¼ºå°‘è¯­ä¹‰å›¾ï¼Œè¿”å›Noneè¡¨ç¤ºæ— æ³•å¤„ç†
            images['semantic'] = semantic_images if len(semantic_images) > 1 else semantic_images[0]
        
        return images
    
    
    def preprocess_inputs(
        self, 
        images: Dict[str, Union[np.ndarray, List[np.ndarray]]], 
        text_prompt: str
    ) -> Dict[str, torch.Tensor]:
        """é¢„å¤„ç†è¾“å…¥æ•°æ®ï¼ˆæ”¯æŒ6æ‘„åƒå¤´å…¨æ™¯ï¼‰"""
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯6æ‘„åƒå¤´æ¨¡å¼
        if isinstance(images['rgb'], list) and len(images['rgb']) == 6:
            # 6æ‘„åƒå¤´å…¨æ™¯æ¨¡å¼
            if self.processor and hasattr(self.processor, 'create_panorama'):
                # åˆ›å»ºå…¨æ™¯å›¾
                rgb_panorama = self.processor.create_panorama(images['rgb'], modality="rgb")
                depth_panorama = None
                semantic_panorama = None
                
                if 'depth' in images and images['depth'] is not None:
                    depth_panorama = self.processor.create_panorama(images['depth'], modality="depth")
                if 'semantic' in images and images['semantic'] is not None:
                    semantic_panorama = self.processor.create_panorama(images['semantic'], modality="semantic")
                
                # å¤„ç†å…¨æ™¯å›¾
                processed_data = self.processor.process(
                    text=text_prompt,
                    rgb_image=rgb_panorama,
                    depth_image=depth_panorama,
                    semantic_image=semantic_panorama,
                    mode="inference"
                )
            else:
                raise ValueError("å¤„ç†å™¨ä¸æ”¯æŒ6æ‘„åƒå¤´å…¨æ™¯æ¨¡å¼")
        else:
            # å•æ‘„åƒå¤´æ¨¡å¼
            processed_data = self.processor.process(
                text=text_prompt,
                rgb_image=images['rgb'],
                depth_image=images.get('depth'),
                semantic_image=images.get('semantic'),
                mode="inference"
            )
        
        # è½¬æ¢ä¸ºå¼ é‡å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
        for key, value in processed_data.items():
            if isinstance(value, torch.Tensor):
                processed_data[key] = value.unsqueeze(0).to(self.config.device)
        
        return processed_data
    
    def load_tmi_features(self, feature_path: str) -> torch.Tensor:
        """åŠ è½½é¢„æå–çš„TMIç‰¹å¾"""
        
        if not Path(feature_path).exists():
            raise FileNotFoundError(f"TMIç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {feature_path}")
        
        features = np.load(feature_path)
        return torch.from_numpy(features).float()
    
    def generate_trajectory(self, inputs: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """ç”Ÿæˆè½¨è¿¹é¢„æµ‹"""
        
        start_time = time.time()
        
        with torch.no_grad():
            # å‡†å¤‡ç”Ÿæˆå‚æ•°
            generation_kwargs = {
                'max_new_tokens': self.config.max_new_tokens,
                'temperature': self.config.temperature if self.config.do_sample else None,
                'top_p': self.config.top_p if self.config.do_sample else None,
                'do_sample': self.config.do_sample,
                'num_beams': self.config.num_beams,
                'repetition_penalty': self.config.repetition_penalty,
                'pad_token_id': self.tokenizer.pad_token_id,
                'eos_token_id': self.tokenizer.eos_token_id,
            }
            
            # å¦‚æœä½¿ç”¨TMIç‰¹å¾ï¼Œæ·»åŠ åˆ°è¾“å…¥ä¸­
            if self.config.use_tmi_features and self.config.tmi_feature_path:
                tmi_features = self.load_tmi_features(self.config.tmi_feature_path)
                inputs['tmi_features'] = tmi_features.unsqueeze(0).to(self.config.device)
            
            # è·å–è¾“å…¥é•¿åº¦
            input_length = inputs['input_ids'].shape[1]
            
            # ç”Ÿæˆ
            with torch.autocast(device_type='cuda' if 'cuda' in self.config.device else 'cpu', enabled=self.config.torch_dtype=='float16'):
                outputs = self.model.generate(
                    **inputs,
                    **generation_kwargs
                )
            
            # è§£ç ç”Ÿæˆçš„éƒ¨åˆ†
            generated_tokens = outputs[0][input_length:]
            generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
            
            generation_time = time.time() - start_time
            
            # æå–è½¨è¿¹
            trajectory = self._extract_trajectory_from_text(generated_text)
            
            return {
                'generated_text': generated_text,
                'trajectory': trajectory,
                'generation_time': generation_time,
                'input_length': input_length,
                'output_length': len(generated_tokens)
            }
    
    def _extract_trajectory_from_text(self, text: str) -> Optional[np.ndarray]:
        """ä»ç”Ÿæˆæ–‡æœ¬ä¸­æå–è½¨è¿¹åæ ‡"""
        
        try:
            import re
            
            # æŸ¥æ‰¾PLANNINGæ ‡ç­¾
            if "<PLANNING>" in text and "</PLANNING>" in text:
                planning_content = text.split("<PLANNING>")[1].split("</PLANNING>")[0]
                
                # åŒ¹é…å¤šç§åæ ‡æ ¼å¼
                # æ ¼å¼1: [x, y, heading]
                pattern1 = r'\[([-+]?\d*\.?\d+),\s*([-+]?\d*\.?\d+),\s*([-+]?\d*\.?\d+)\]'
                matches = re.findall(pattern1, planning_content)
                
                if matches:
                    trajectory = np.array([[float(x), float(y), float(h)] for x, y, h in matches])
                    return trajectory
                
                # æ ¼å¼2: [x: 1.23, y: 4.56]
                pattern2 = r'\[x:\s*([-+]?\d*\.?\d+),\s*y:\s*([-+]?\d*\.?\d+)\]'
                matches = re.findall(pattern2, planning_content)
                
                if matches:
                    trajectory = np.array([[float(x), float(y)] for x, y in matches])
                    return trajectory
            
            return None
            
        except Exception as e:
            if self.config.verbose:
                print(f"è½¨è¿¹æå–å¤±è´¥: {e}")
            return None
    
    def _validate_trajectory(self, trajectory: np.ndarray) -> bool:
        """éªŒè¯è½¨è¿¹åˆç†æ€§"""
        
        if trajectory is None or len(trajectory) == 0:
            return False
        
        # æ£€æŸ¥å½¢çŠ¶
        if len(trajectory.shape) != 2 or trajectory.shape[1] < 2:
            return False
        
        # æ£€æŸ¥æ•°å€¼èŒƒå›´ï¼ˆå‡è®¾å•ä½æ˜¯ç±³ï¼‰
        if np.any(np.abs(trajectory) > 1000):  # è¶…è¿‡1kmè®¤ä¸ºä¸åˆç†
            return False
        
        # æ£€æŸ¥è¿ç»­æ€§ï¼ˆç›¸é‚»ç‚¹è·ç¦»ä¸åº”è¿‡å¤§ï¼‰
        if len(trajectory) > 1:
            diffs = np.diff(trajectory, axis=0)
            distances = np.linalg.norm(diffs, axis=1)
            if np.any(distances > 50):  # ç›¸é‚»ç‚¹è·ç¦»è¶…è¿‡50mè®¤ä¸ºä¸åˆç†
                return False
        
        return True
    
    def single_inference(
        self,
        rgb_paths: Union[str, List[str]],
        depth_paths: Optional[Union[str, List[str]]] = None,
        semantic_paths: Optional[Union[str, List[str]]] = None,
        text_prompt: Optional[str] = None
    ) -> Dict[str, Any]:
        """å•æ ·æœ¬æ¨ç†ï¼ˆæ”¯æŒ6æ‘„åƒå¤´ï¼‰"""
        
        if not self.model:
            self.setup_model()
        
        if text_prompt is None:
            text_prompt = self.config.text_prompt
        
        # åŠ è½½å›¾åƒ
        images = self.load_images(rgb_paths, depth_paths, semantic_paths)
        
        if images is None:
            if self.config.verbose:
                print("è­¦å‘Š: ç¼ºå°‘å¿…è¦çš„æ¨¡æ€æ•°æ®ï¼Œè·³è¿‡è¯¥æ ·æœ¬")
            return None
        
        # é¢„å¤„ç†
        inputs = self.preprocess_inputs(images, text_prompt)
        
        # ç”Ÿæˆ
        results = self.generate_trajectory(inputs)
        
        # æ·»åŠ è¾“å…¥ä¿¡æ¯
        results.update({
            'rgb_paths': rgb_paths if isinstance(rgb_paths, list) else [rgb_paths],
            'depth_paths': depth_paths if isinstance(depth_paths, list) else [depth_paths] if depth_paths else None,
            'semantic_paths': semantic_paths if isinstance(semantic_paths, list) else [semantic_paths] if semantic_paths else None,
            'text_prompt': text_prompt,
            'images': images
        })
        
        return results
    
    def batch_inference(self, input_dir: str) -> List[Dict[str, Any]]:
        """æ‰¹é‡æ¨ç†"""
        
        if not self.model:
            self.setup_model()
        
        input_path = Path(input_dir)
        if not input_path.exists():
            raise FileNotFoundError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        
        # æŸ¥æ‰¾RGBå›¾åƒæ–‡ä»¶
        rgb_files = list(input_path.glob("*_rgb.*")) + list(input_path.glob("*.jpg")) + list(input_path.glob("*.png"))
        
        if not rgb_files:
            raise FileNotFoundError(f"åœ¨ {input_dir} ä¸­æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶")
        
        results = []
        
        for rgb_file in rgb_files:
            try:
                # æ„å»ºå¯¹åº”çš„æ·±åº¦å’Œè¯­ä¹‰æ–‡ä»¶è·¯å¾„
                base_name = rgb_file.stem.replace("_rgb", "")
                depth_file = input_path / f"{base_name}_depth{rgb_file.suffix}"
                semantic_file = input_path / f"{base_name}_semantic{rgb_file.suffix}"
                
                # æ¨ç†
                result = self.single_inference(
                    rgb_path=str(rgb_file),
                    depth_path=str(depth_file) if depth_file.exists() else None,
                    semantic_path=str(semantic_file) if semantic_file.exists() else None
                )
                
                results.append(result)
                
                if self.config.verbose:
                    print(f"å¤„ç†å®Œæˆ: {rgb_file.name}")
                
            except Exception as e:
                if self.config.verbose:
                    print(f"å¤„ç†å¤±è´¥ {rgb_file.name}: {e}")
                continue
        
        return results
    
    def save_results(self, results: Union[Dict, List[Dict]], output_name: str = "inference_results"):
        """ä¿å­˜æ¨ç†ç»“æœ"""
        
        if isinstance(results, dict):
            results = [results]
        
        # ä¿å­˜JSONç»“æœ
        json_results = []
        for i, result in enumerate(results):
            json_result = {
                'index': i,
                'rgb_path': result.get('rgb_path'),
                'depth_path': result.get('depth_path'),
                'semantic_path': result.get('semantic_path'),
                'text_prompt': result.get('text_prompt'),
                'generated_text': result.get('generated_text'),
                'trajectory': result.get('trajectory').tolist() if result.get('trajectory') is not None else None,
                'generation_time': result.get('generation_time'),
                'input_length': result.get('input_length'),
                'output_length': result.get('output_length')
            }
            json_results.append(json_result)
        
        json_file = Path(self.config.output_dir) / f"{output_name}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_results, f, indent=2, ensure_ascii=False)
        
        if self.config.verbose:
            print(f"ç»“æœå·²ä¿å­˜åˆ°: {json_file}")
        
        # ä¿å­˜å¯è§†åŒ–ç»“æœ
        if self.config.save_visualization and self.visualizer:
            self.create_visualizations(results, output_name)
        
        return json_file
    
    def create_visualizations(self, results: List[Dict], output_name: str):
        """åˆ›å»ºå¯è§†åŒ–ç»“æœ"""
        
        if not plt:
            if self.config.verbose:
                print("matplotlibä¸å¯ç”¨ï¼Œè·³è¿‡å¯è§†åŒ–")
            return
        
        try:
            # ä¸ºæ¯ä¸ªç»“æœåˆ›å»ºå¯è§†åŒ–
            for i, result in enumerate(results):
                if result.get('trajectory') is None:
                    continue
                
                fig, axes = plt.subplots(1, 4, figsize=(20, 5))
                
                # 1. RGBå›¾åƒ
                if 'images' in result and 'rgb' in result['images']:
                    axes[0].imshow(result['images']['rgb'])
                    axes[0].set_title('RGB Image')
                    axes[0].axis('off')
                
                # 2. æ·±åº¦å›¾åƒ
                if 'images' in result and 'depth' in result['images']:
                    axes[1].imshow(result['images']['depth'], cmap='viridis')
                    axes[1].set_title('Depth Image')
                    axes[1].axis('off')
                
                # 3. è¯­ä¹‰åˆ†å‰²å›¾åƒ
                if 'images' in result and 'semantic' in result['images']:
                    axes[2].imshow(result['images']['semantic'], cmap='tab20')
                    axes[2].set_title('Semantic Image')
                    axes[2].axis('off')
                
                # 4. é¢„æµ‹è½¨è¿¹
                trajectory = result['trajectory']
                axes[3].plot(trajectory[:, 0], trajectory[:, 1], 'b-', linewidth=2, marker='o')
                axes[3].plot(trajectory[0, 0], trajectory[0, 1], 'go', markersize=8, label='èµ·ç‚¹')
                axes[3].plot(trajectory[-1, 0], trajectory[-1, 1], 'ro', markersize=8, label='ç»ˆç‚¹')
                axes[3].set_xlabel('X (meters)')
                axes[3].set_ylabel('Y (meters)')
                axes[3].set_title('Predicted Trajectory')
                axes[3].legend()
                axes[3].grid(True, alpha=0.3)
                axes[3].axis('equal')
                
                plt.tight_layout()
                
                # ä¿å­˜å›¾åƒ
                output_file = Path(self.config.output_dir) / f"{output_name}_sample_{i}.png"
                plt.savefig(output_file, dpi=300, bbox_inches='tight')
                plt.close()
                
                if self.config.verbose:
                    print(f"å¯è§†åŒ–å·²ä¿å­˜: {output_file}")
        
        except Exception as e:
            if self.config.verbose:
                print(f"åˆ›å»ºå¯è§†åŒ–æ—¶å‡ºé”™: {e}")
    
    def interactive_mode(self):
        """äº¤äº’å¼æ¨ç†æ¨¡å¼"""
        
        if not self.model:
            self.setup_model()
        
        print("\n=== ä¸‰æ¨¡æ€VLMäº¤äº’å¼æ¨ç† ===")
        print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯")
        
        while True:
            try:
                print("\n" + "="*50)
                
                # è·å–ç”¨æˆ·è¾“å…¥
                rgb_path = input("RGBå›¾åƒè·¯å¾„: ").strip()
                
                if rgb_path.lower() in ['quit', 'exit']:
                    print("é€€å‡ºäº¤äº’æ¨¡å¼")
                    break
                
                if rgb_path.lower() == 'help':
                    self._print_help()
                    continue
                
                if not rgb_path or not Path(rgb_path).exists():
                    print("âŒ æ— æ•ˆçš„RGBå›¾åƒè·¯å¾„")
                    continue
                
                # å¯é€‰çš„æ·±åº¦å’Œè¯­ä¹‰å›¾åƒ
                depth_path = input("æ·±åº¦å›¾åƒè·¯å¾„ (å¯é€‰): ").strip()
                if not depth_path or not Path(depth_path).exists():
                    depth_path = None
                
                semantic_path = input("è¯­ä¹‰å›¾åƒè·¯å¾„ (å¯é€‰): ").strip()
                if not semantic_path or not Path(semantic_path).exists():
                    semantic_path = None
                
                # è‡ªå®šä¹‰æç¤ºè¯
                custom_prompt = input(f"è‡ªå®šä¹‰æç¤ºè¯ (é»˜è®¤: {self.config.text_prompt}): ").strip()
                text_prompt = custom_prompt if custom_prompt else self.config.text_prompt
                
                # æ‰§è¡Œæ¨ç†
                print("\nğŸš€ å¼€å§‹æ¨ç†...")
                result = self.single_inference(rgb_path, depth_path, semantic_path, text_prompt)
                
                # æ˜¾ç¤ºç»“æœ
                self._display_result(result)
                
                # ä¿å­˜ç»“æœ
                save_result = input("\næ˜¯å¦ä¿å­˜ç»“æœ? (y/n): ").strip().lower()
                if save_result == 'y':
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    self.save_results(result, f"interactive_{timestamp}")
                
            except KeyboardInterrupt:
                print("\n\nç”¨æˆ·ä¸­æ–­ï¼Œé€€å‡ºäº¤äº’æ¨¡å¼")
                break
            except Exception as e:
                print(f"\nâŒ æ¨ç†å¤±è´¥: {e}")
                continue
    
    def _print_help(self):
        """æ‰“å°å¸®åŠ©ä¿¡æ¯"""
        print("\n=== å¸®åŠ©ä¿¡æ¯ ===")
        print("1. RGBå›¾åƒè·¯å¾„: å¿…éœ€ï¼Œæ”¯æŒjpgã€pngç­‰æ ¼å¼")
        print("2. æ·±åº¦å›¾åƒè·¯å¾„: å¯é€‰ï¼Œå¦‚ä¸æä¾›å°†è‡ªåŠ¨ç”Ÿæˆæ¨¡æ‹Ÿæ·±åº¦å›¾")
        print("3. è¯­ä¹‰å›¾åƒè·¯å¾„: å¯é€‰ï¼Œå¦‚ä¸æä¾›å°†è‡ªåŠ¨ç”Ÿæˆæ¨¡æ‹Ÿè¯­ä¹‰å›¾")
        print("4. è‡ªå®šä¹‰æç¤ºè¯: å¯é€‰ï¼Œç”¨äºæŒ‡å¯¼æ¨¡å‹ç”Ÿæˆç‰¹å®šç±»å‹çš„è½¨è¿¹")
        print("5. æ”¯æŒçš„å‘½ä»¤:")
        print("   - 'help': æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯")
        print("   - 'quit' æˆ– 'exit': é€€å‡ºäº¤äº’æ¨¡å¼")
        print("================")
    
    def _display_result(self, result: Dict[str, Any]):
        """æ˜¾ç¤ºæ¨ç†ç»“æœ"""
        print("\nğŸ“Š æ¨ç†ç»“æœ:")
        print(f"  ç”Ÿæˆæ—¶é—´: {result['generation_time']:.3f} ç§’")
        print(f"  è¾“å…¥é•¿åº¦: {result['input_length']} tokens")
        print(f"  è¾“å‡ºé•¿åº¦: {result['output_length']} tokens")
        
        print(f"\nğŸ’¬ ç”Ÿæˆæ–‡æœ¬:")
        print(f"  {result['generated_text']}")
        
        if result['trajectory'] is not None:
            trajectory = result['trajectory']
            print(f"\nğŸ›£ï¸  é¢„æµ‹è½¨è¿¹ ({len(trajectory)} ä¸ªç‚¹):")
            for i, point in enumerate(trajectory[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ªç‚¹
                print(f"    ç‚¹{i+1}: x={point[0]:.2f}, y={point[1]:.2f}")
            if len(trajectory) > 5:
                print(f"    ... (è¿˜æœ‰ {len(trajectory)-5} ä¸ªç‚¹)")
        else:
            print("\nâŒ æœªæ‰¾åˆ°æœ‰æ•ˆè½¨è¿¹")


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    
    parser = argparse.ArgumentParser(description="ä¸‰æ¨¡æ€VLMæ¨ç†è„šæœ¬")
    
    # æ¨¡å‹é…ç½®
    parser.add_argument("--model_path", type=str, required=True,
                       help="è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„")
    parser.add_argument("--model_config_path", type=str, default=None,
                       help="æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„")
    
    # æ¨ç†æ¨¡å¼
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--rgb_image", type=str,
                      help="å•å¼ RGBå›¾åƒè·¯å¾„")
    group.add_argument("--batch_input_dir", type=str,
                      help="æ‰¹é‡è¾“å…¥ç›®å½•")
    group.add_argument("--interactive", action="store_true",
                      help="äº¤äº’å¼æ¨ç†æ¨¡å¼")
    
    # TMIç‰¹å¾æ¨¡å¼
    parser.add_argument("--use_tmi_features", action="store_true",
                       help="ä½¿ç”¨TMIç‰¹å¾æ¨¡å¼ï¼ˆç”¨äºLLaMA Factoryè®­ç»ƒçš„æ¨¡å‹ï¼‰")
    parser.add_argument("--tmi_feature_path", type=str, default=None,
                       help="TMIç‰¹å¾æ–‡ä»¶è·¯å¾„ï¼ˆ.npyæ ¼å¼ï¼‰")
    parser.add_argument("--tmi_checkpoint", type=str, default=None,
                       help="TMIæ¨¡å—checkpointè·¯å¾„ï¼ˆç”¨äºåœ¨çº¿æå–ç‰¹å¾ï¼‰")
    
    # è¾“å…¥æ–‡ä»¶ï¼ˆå•æ ·æœ¬æ¨¡å¼ï¼‰
    parser.add_argument("--depth_image", type=str, default=None,
                       help="æ·±åº¦å›¾åƒè·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--semantic_image", type=str, default=None,
                       help="è¯­ä¹‰å›¾åƒè·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--text_prompt", type=str,
                       default="åŸºäºä¸‰æ¨¡æ€æ„ŸçŸ¥ä¿¡æ¯ï¼Œé¢„æµ‹è½¦è¾†çš„æœªæ¥è½¨è¿¹ã€‚",
                       help="æ–‡æœ¬æç¤ºè¯")
    
    # è¾“å‡ºé…ç½®
    parser.add_argument("--output_dir", type=str, default="./inference_results",
                       help="è¾“å‡ºç›®å½•")
    
    # ç”Ÿæˆé…ç½®
    parser.add_argument("--max_new_tokens", type=int, default=512,
                       help="æœ€å¤§ç”Ÿæˆtokenæ•°")
    parser.add_argument("--temperature", type=float, default=0.0,
                       help="ç”Ÿæˆæ¸©åº¦")
    parser.add_argument("--top_p", type=float, default=1.0,
                       help="nucleusé‡‡æ ·å‚æ•°")
    parser.add_argument("--do_sample", action="store_true",
                       help="æ˜¯å¦ä½¿ç”¨é‡‡æ ·")
    parser.add_argument("--num_beams", type=int, default=1,
                       help="æŸæœç´¢beamæ•°é‡")
    parser.add_argument("--repetition_penalty", type=float, default=1.0,
                       help="é‡å¤æƒ©ç½š")
    
    # å¤„ç†é…ç½®
    parser.add_argument("--max_length", type=int, default=2048,
                       help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--image_size", type=int, default=392,
                       help="å›¾åƒå°ºå¯¸")
    
    # å¯è§†åŒ–é…ç½®
    parser.add_argument("--save_visualization", action="store_true",
                       help="æ˜¯å¦ä¿å­˜å¯è§†åŒ–ç»“æœ")
    parser.add_argument("--show_attention", action="store_true",
                       help="æ˜¯å¦æ˜¾ç¤ºæ³¨æ„åŠ›æƒé‡")
    
    # æ€§èƒ½é…ç½®
    parser.add_argument("--device", type=str, default="auto",
                       choices=["auto", "cuda", "cpu"],
                       help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--torch_dtype", type=str, default="float16",
                       choices=["float16", "float32"],
                       help="æ¨¡å‹æ•°æ®ç±»å‹")
    parser.add_argument("--use_compile", action="store_true",
                       help="æ˜¯å¦ä½¿ç”¨PyTorch 2.0ç¼–è¯‘")
    
    # å…¶ä»–é…ç½®
    parser.add_argument("--seed", type=int, default=42,
                       help="éšæœºç§å­")
    parser.add_argument("--verbose", action="store_true",
                       help="è¯¦ç»†è¾“å‡º")
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    
    args = parse_args()
    
    # è®¾ç½®è®¾å¤‡
    if args.device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    else:
        device = args.device
    
    # åˆ›å»ºæ¨ç†é…ç½®
    inference_config = InferenceConfig(
        model_path=args.model_path,
        model_config_path=args.model_config_path,
        use_tmi_features=args.use_tmi_features,
        tmi_feature_path=args.tmi_feature_path,
        tmi_checkpoint=args.tmi_checkpoint,
        rgb_image_path=args.rgb_image,
        depth_image_path=args.depth_image,
        semantic_image_path=args.semantic_image,
        text_prompt=args.text_prompt,
        batch_input_dir=args.batch_input_dir,
        output_dir=args.output_dir,
        max_new_tokens=args.max_new_tokens,
        temperature=args.temperature,
        top_p=args.top_p,
        do_sample=args.do_sample,
        num_beams=args.num_beams,
        repetition_penalty=args.repetition_penalty,
        max_length=args.max_length,
        image_size=args.image_size,
        save_visualization=args.save_visualization,
        show_attention=args.show_attention,
        interactive_mode=args.interactive,
        device=device,
        torch_dtype=args.torch_dtype,
        use_compile=args.use_compile,
        use_flash_attention=getattr(args, 'use_flash_attention', True),
        use_better_transformer=getattr(args, 'use_better_transformer', True),
        compute_metrics=getattr(args, 'compute_metrics', False),
        ground_truth_path=getattr(args, 'ground_truth_path', None),
        profile=getattr(args, 'profile', False),
        seed=args.seed,
        verbose=args.verbose
    )
    
    # æ‰“å°é…ç½®
    if args.verbose:
        print("=== æ¨ç†é…ç½® ===")
        for key, value in inference_config.__dict__.items():
            print(f"{key}: {value}")
        print("===============")
    
    try:
        # åˆ›å»ºæ¨ç†å™¨
        inference_engine = TriModalInference(inference_config)
        
        if args.interactive:
            # äº¤äº’å¼æ¨¡å¼
            inference_engine.interactive_mode()
        elif args.batch_input_dir:
            # æ‰¹é‡æ¨ç†
            print("å¼€å§‹æ‰¹é‡æ¨ç†...")
            results = inference_engine.batch_inference(args.batch_input_dir)
            inference_engine.save_results(results, "batch_inference")
            print(f"æ‰¹é‡æ¨ç†å®Œæˆï¼Œå¤„ç†äº† {len(results)} ä¸ªæ ·æœ¬")
        else:
            # å•æ ·æœ¬æ¨ç†
            print("å¼€å§‹å•æ ·æœ¬æ¨ç†...")
            result = inference_engine.single_inference(
                rgb_path=args.rgb_image,
                depth_path=args.depth_image,
                semantic_path=args.semantic_image,
                text_prompt=args.text_prompt
            )
            inference_engine.save_results(result, "single_inference")
            print("å•æ ·æœ¬æ¨ç†å®Œæˆ")
        
        print("\nâœ“ æ¨ç†å®Œæˆ!")
        return 0
        
    except KeyboardInterrupt:
        print("\næ¨ç†è¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"\nâœ— æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())