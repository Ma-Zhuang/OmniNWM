#!/usr/bin/env python3
"""
å¤šGPUå¹¶è¡Œç”Ÿæˆæ·±åº¦å’Œè¯­ä¹‰å›¾
å®Œå…¨ç‹¬ç«‹çš„è„šæœ¬ï¼Œé¿å…å¾ªç¯å¯¼å…¥é—®é¢˜
"""

import os
import sys
import argparse
import logging
import pickle
from pathlib import Path
from datetime import datetime
import torch
import numpy as np
from PIL import Image
from typing import List, Tuple
import multiprocessing as mp
from tqdm import tqdm
import yaml

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

def setup_logging(gpu_id: int = None):
    """è®¾ç½®æ—¥å¿—"""
    gpu_suffix = f"_gpu{gpu_id}" if gpu_id is not None else ""
    log_file = f"multi_gpu_generation{gpu_suffix}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    logger = logging.getLogger(f"GPU_{gpu_id}" if gpu_id is not None else "MAIN")
    logger.setLevel(logging.INFO)
    
    # æ¸…é™¤å·²æœ‰çš„å¤„ç†å™¨
    logger.handlers = []
    
    # æ–‡ä»¶å¤„ç†å™¨
    fh = logging.FileHandler(log_file)
    fh.setLevel(logging.INFO)
    
    # æ§åˆ¶å°å¤„ç†å™¨
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    
    # æ ¼å¼åŒ–å™¨
    formatter = logging.Formatter(
        f'%(asctime)s - GPU{gpu_id if gpu_id is not None else "MAIN"} - %(levelname)s - %(message)s'
    )
    fh.setFormatter(formatter)
    ch.setFormatter(formatter)
    
    logger.addHandler(fh)
    logger.addHandler(ch)
    
    return logger

def process_batch_on_gpu(args):
    """åœ¨æŒ‡å®šGPUä¸Šå¤„ç†ä¸€æ‰¹å›¾åƒ"""
    gpu_id, image_batch, config_path, output_dir = args
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(gpu_id)
    logger.info(f"GPU {gpu_id} å¼€å§‹å¤„ç† {len(image_batch)} å¼ å›¾åƒ")
    
    # å¯¼å…¥å¿…è¦çš„æ¨¡å—
    import torch
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # è¯»å–é…ç½®
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    from src.processors.depth_processor import DepthProcessor
    from src.processors.semantic_processor import SemanticProcessor
    
    depth_config = config.get('depth', {})
    depth_processor = DepthProcessor(
        model_name=depth_config.get('model_name', 'DPT'),
        model_path=depth_config.get('model_path'),
        device=device,
        use_local_weights=True if depth_config.get('model_path') else False
    )
    logger.info(f"GPU {gpu_id}: æ·±åº¦å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
    
    semantic_config = config.get('semantic', {})
    semantic_processor = SemanticProcessor(
        model_name=semantic_config.get('model_name', 'SegFormer'),
        model_path=semantic_config.get('model_path'),
        device=device,
        use_local_weights=True if semantic_config.get('model_path') else False
    )
    logger.info(f"GPU {gpu_id}: è¯­ä¹‰å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if output_dir:
        output_path = Path(output_dir)
    else:
        output_path = Path(config['dataset']['output_directory'])
    
    depth_dir = output_path / 'depth'
    semantic_dir = output_path / 'semantic'
    depth_dir.mkdir(parents=True, exist_ok=True)
    semantic_dir.mkdir(parents=True, exist_ok=True)
    
    dataroot = Path(config['dataset']['nuscenes_dataroot'])
    
    success_count = 0
    error_count = 0
    
    # å¤„ç†æ¯å¼ å›¾åƒ
    for image_path, image_id in tqdm(image_batch, desc=f"GPU {gpu_id}", position=gpu_id):
        try:
            # æ„å»ºè¾“å‡ºè·¯å¾„
            rel_path = Path(image_path).relative_to(dataroot)
            depth_path = depth_dir / rel_path.parent / f"{rel_path.stem}_depth.png"
            semantic_path = semantic_dir / rel_path.parent / f"{rel_path.stem}_semantic.png"
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            depth_path.parent.mkdir(parents=True, exist_ok=True)
            semantic_path.parent.mkdir(parents=True, exist_ok=True)
            
            # è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶
            if depth_path.exists() and semantic_path.exists():
                success_count += 1
                continue
            
            # è¯»å–å›¾åƒ
            image = Image.open(image_path).convert('RGB')
            
            # ç”Ÿæˆæ·±åº¦å›¾
            if not depth_path.exists():
                depth_map = depth_processor.process(image)
                # æ£€æŸ¥æ˜¯å¦è¿”å›äº†æœ‰æ•ˆçš„æ·±åº¦å›¾
                if depth_map is None or np.all(depth_map == 0):
                    logger.error(f"GPU {gpu_id}: æ·±åº¦å›¾ç”Ÿæˆå¤±è´¥ï¼ˆè¿”å›ç©ºå€¼ï¼‰: {image_path}")
                    error_count += 1
                    continue
                # ä¿å­˜ä¸º16ä½PNGï¼ˆæ¯«ç±³å•ä½ï¼‰
                depth_mm = np.clip(depth_map * 1000, 0, 65535).astype(np.uint16)
                Image.fromarray(depth_mm).save(depth_path)
            
            # ç”Ÿæˆè¯­ä¹‰å›¾
            if not semantic_path.exists():
                semantic_map = semantic_processor.process(image)
                # æ£€æŸ¥æ˜¯å¦è¿”å›äº†æœ‰æ•ˆçš„è¯­ä¹‰å›¾
                if semantic_map is None or np.all(semantic_map == 0):
                    logger.error(f"GPU {gpu_id}: è¯­ä¹‰å›¾ç”Ÿæˆå¤±è´¥ï¼ˆè¿”å›ç©ºå€¼ï¼‰: {image_path}")
                    error_count += 1
                    continue
                # ä¿å­˜ä¸º8ä½ç´¢å¼•å›¾
                Image.fromarray(semantic_map.astype(np.uint8)).save(semantic_path)
            
            success_count += 1
            
        except Exception as e:
            logger.error(f"GPU {gpu_id} å¤„ç†å¤±è´¥ {image_path}: {e}")
            error_count += 1
        
        # å®šæœŸæ¸…ç†GPUç¼“å­˜
        if (success_count + error_count) % 100 == 0:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    logger.info(f"GPU {gpu_id} å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {error_count}")
    return success_count, error_count

def collect_all_images(config_path: str, max_samples: int = None) -> List[Tuple[str, str]]:
    """æ”¶é›†æ‰€æœ‰éœ€è¦å¤„ç†çš„å›¾åƒ"""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    from nuscenes.nuscenes import NuScenes
    
    print("åŠ è½½nuScenesæ•°æ®é›†...")
    nusc = NuScenes(
        version=config['dataset']['nuscenes_version'],
        dataroot=config['dataset']['nuscenes_dataroot'],
        verbose=False
    )
    
    # åŠ è½½å·²å¤„ç†çš„å›¾åƒ
    output_dir = Path(config['dataset']['output_directory'])
    progress_file = output_dir / 'generation_progress.pkl'
    processed_images = set()
    
    if progress_file.exists():
        with open(progress_file, 'rb') as f:
            processed_images = pickle.load(f)
        print(f"å·²åŠ è½½è¿›åº¦: {len(processed_images)} å¼ å›¾åƒå·²å¤„ç†")
    
    image_info = []
    sample_count = 0
    camera_channels = config['cameras']['channels']
    dataroot = Path(config['dataset']['nuscenes_dataroot'])
    
    for scene in tqdm(nusc.scene, desc="æ”¶é›†å›¾åƒè·¯å¾„"):
        sample_token = scene['first_sample_token']
        
        while sample_token:
            sample = nusc.get('sample', sample_token)
            
            for camera in camera_channels:
                if camera in sample['data']:
                    sample_data = nusc.get('sample_data', sample['data'][camera])
                    image_path = str(dataroot / sample_data['filename'])
                    
                    # è·³è¿‡å·²å¤„ç†çš„å›¾åƒ
                    if image_path not in processed_images:
                        image_info.append((image_path, f"{sample_token}_{camera}"))
            
            sample_token = sample['next']
            sample_count += 1
            
            if max_samples and sample_count >= max_samples:
                break
        
        if max_samples and sample_count >= max_samples:
            break
    
    print(f"æ‰¾åˆ° {len(image_info)} å¼ å¾…å¤„ç†å›¾åƒ")
    return image_info

def main():
    parser = argparse.ArgumentParser(description='å¤šGPUå¹¶è¡Œç”Ÿæˆæ·±åº¦å’Œè¯­ä¹‰å›¾')
    parser.add_argument('--config', type=str, 
                       default='configs/production_config.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', type=str, default=None,
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--num-gpus', type=int, default=4,
                       help='ä½¿ç”¨çš„GPUæ•°é‡')
    parser.add_argument('--max-samples', type=int, default=None,
                       help='æœ€å¤§å¤„ç†æ ·æœ¬æ•°')
    
    args = parser.parse_args()
    
    # è®¾ç½®multiprocessingä½¿ç”¨spawnæ–¹å¼
    mp.set_start_method('spawn', force=True)
    
    # æ”¶é›†æ‰€æœ‰å›¾åƒ
    all_images = collect_all_images(args.config, args.max_samples)
    
    if not all_images:
        print("âœ… æ‰€æœ‰å›¾åƒå·²å¤„ç†å®Œæˆï¼")
        return
    
    # æ£€æŸ¥å¯ç”¨GPUæ•°é‡
    available_gpus = torch.cuda.device_count()
    num_gpus = min(args.num_gpus, available_gpus)
    print(f"\nä½¿ç”¨ {num_gpus} ä¸ªGPU (å¯ç”¨: {available_gpus})")
    
    # å°†å›¾åƒåˆ†é…åˆ°å„ä¸ªGPU
    images_per_gpu = len(all_images) // num_gpus
    gpu_tasks = []
    
    for i in range(num_gpus):
        start_idx = i * images_per_gpu
        if i == num_gpus - 1:
            batch = all_images[start_idx:]
        else:
            batch = all_images[start_idx:start_idx + images_per_gpu]
        
        gpu_tasks.append((i, batch, args.config, args.output_dir))
    
    print(f"æ€»å…± {len(all_images)} å¼ å›¾åƒï¼Œæ¯ä¸ªGPUå¤„ç†çº¦ {images_per_gpu} å¼ ")
    print("="*70)
    print("ğŸš€ å¼€å§‹å¤šGPUå¹¶è¡Œå¤„ç†")
    print("="*70)
    
    # ä½¿ç”¨multiprocessing.Poolå¹¶è¡Œå¤„ç†
    with mp.Pool(processes=num_gpus) as pool:
        results = pool.map(process_batch_on_gpu, gpu_tasks)
    
    # ç»Ÿè®¡ç»“æœ
    total_success = sum(r[0] for r in results)
    total_error = sum(r[1] for r in results)
    
    print("\n" + "="*70)
    print("ğŸ“Š å¤šGPUå¤„ç†å®Œæˆç»Ÿè®¡:")
    print("="*70)
    
    for i, (success, error) in enumerate(results):
        print(f"GPU {i}: æˆåŠŸ {success}, å¤±è´¥ {error}")
    
    print(f"\næ€»è®¡: æˆåŠŸ {total_success}, å¤±è´¥ {total_error}")
    
    if total_success > 0:
        print("\nâœ… æ·±åº¦å’Œè¯­ä¹‰å›¾ç”Ÿæˆå®Œæˆï¼")
        print("\nä¸‹ä¸€æ­¥: è¿è¡ŒShareGPTæ•°æ®é›†ç”Ÿæˆ")
        print("python scripts/build_sharegpt_dataset.py --config configs/sharegpt_dataset_config.yaml")

if __name__ == "__main__":
    main()